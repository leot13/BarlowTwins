{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Barlow Twins Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ioKMz2UE-50_",
        "0LGwfhxG_Ejj"
      ],
      "machine_shape": "hm",
      "mount_file_id": "15FhpvJvHpuQGlBV53R8JYRf6venOxJO2",
      "authorship_tag": "ABX9TyOJP9+te7tdcfVdfTt27jSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leot13/BarlowTwins/blob/main/Barlow_Twins_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioKMz2UE-50_"
      },
      "source": [
        "## Installing required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ta6bg9LK0r1"
      },
      "source": [
        "!pip install -U albumentations -q\n",
        "!pip install wandb --upgrade -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LGwfhxG_Ejj"
      },
      "source": [
        "## Importing dependencies and dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQoMqv_2qh4M"
      },
      "source": [
        "#Login to wandb. Get the API key in wandb settings\n",
        "!wandb login --relogin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20J0jNKFz1xJ"
      },
      "source": [
        "import wandb\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import config \n",
        "from training import train_BT, train_FT\n",
        "from model import (BarlowTwins, loss_fun, BarlowTwins_FT)\n",
        "from utils import (CIFARDataset, makeTransforms, makeTransforms_Fine_Tuning,\n",
        "                   compute_accuracy, save_checkpoint, load_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFfIJglWimIl"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=None)\n",
        "valset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akm8xOcP_JTn"
      },
      "source": [
        "## Self-supervised learning set-up and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CbNxWX2nIes"
      },
      "source": [
        "#Get the transforms\n",
        "train_transform1, train_transform2 = makeTransforms(config.IMG_HEIGHT, config.IMG_WIDTH)\n",
        "\n",
        "#Create the datasets and dataloaders using the transforms\n",
        "train_data = CIFARDataset(trainset, transform1= train_transform1, transform2=  train_transform2)\n",
        "val_data = CIFARDataset(valset, transform1= train_transform1, transform2=  train_transform2)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = config.BATCH_SIZE, shuffle= True )\n",
        "val_loader = DataLoader(val_data, batch_size = config.BATCH_SIZE, shuffle= True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoC0gScM4yA2"
      },
      "source": [
        "#Setting up all the model's parameters\n",
        "model = BarlowTwins(config.IN_FEATURES, config.Z_DIM).to(config.DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(),lr = config.LR) \n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "if config.LOAD_CHECKPOINT:\n",
        "  load_checkpoint(config.CHECKPOINT_FILENAME, model, optimizer, lr= config.LR)\n",
        "  model = model.to(config.DEVICE)\n",
        "\n",
        "#Start the run on wandb. Here the entity should be your wandb name\n",
        "wandb.init(project=config.PROJECT_NAME, entity=\"tronchonleo\")\n",
        "wandb.watch(model, loss_fun, log=\"all\", log_freq=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Jg7vD94Kcy"
      },
      "source": [
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(config.NUM_EPOCHS):\n",
        "\n",
        "  #Train and return losses\n",
        "  loss, avg_val_loss = train_BT(train_loader, val_loader, model, optimizer, config.DEVICE, scaler, config.LAMBDA)\n",
        "\n",
        "  #Display results\n",
        "  print(f\"Epoch: {epoch}, Loss: {loss.item()} Val Loss: {avg_val_loss.item()}\")\n",
        "  wandb.log({\"loss\": loss.item(), \"val_loss\": avg_val_loss.item(), \"lr\": optimizer.param_groups[0]['lr']})\n",
        "\n",
        "  #Update learning rate scheduler\n",
        "  scheduler.step(avg_val_loss)\n",
        "\n",
        "  #When the model improves, save checkpoint and update the best validation loss \n",
        "  if (config.SAVE_CHECKPOINT and best_val_loss > avg_val_loss.item()):\n",
        "    best_val_loss = avg_val_loss.item()\n",
        "    save_checkpoint(model, optimizer, filename=config.CHECKPOINT_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aquLYMeH4g6"
      },
      "source": [
        "## Evaluation set-up an training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M0VVDJqJk8o"
      },
      "source": [
        "#Get the fine tuning transforms\n",
        "train_transform1, train_transform2 = makeTransforms_Fine_Tuning(config.IMG_HEIGHT, config.IMG_WIDTH)\n",
        "\n",
        "#Create the datasets and dataloaders using the transforms\n",
        "train_data = CIFARDataset(trainset, transform1= train_transform1, transform2=  train_transform2)\n",
        "val_data = CIFARDataset(valset, transform1= train_transform1, transform2=  train_transform2)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size = config.BATCH_SIZE, shuffle= True )\n",
        "val_loader = DataLoader(val_data, batch_size = config.BATCH_SIZE, shuffle= True )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y48CQdRv6dZB"
      },
      "source": [
        "barlow_twins = BarlowTwins(config.IN_FEATURES, config.Z_DIM)\n",
        "bt_optimizer = optim.Adam(barlow_twins.parameters(), lr = config.LR) \n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "#Load the self-supervised BarlowTwins and create the fine tuning model\n",
        "load_checkpoint(config.CHECKPOINT_FILENAME, barlow_twins, bt_optimizer, lr= config.LR)\n",
        "ft_model = BarlowTwins_FT(barlow_twins, config.Z_DIM, num_cat= config.NUM_CAT).to(config.DEVICE)\n",
        "optimizer = optim.Adam(ft_model.parameters(), lr = config.LR) \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Freeze the BarlowTwins' parameters\n",
        "ft_model.bt.requires_grad_(False)\n",
        "ft_model.linear.requires_grad_(True)\n",
        "\n",
        "#Start the run on wandb. Here the entity should be your wandb name\n",
        "wandb.init(project=config.PROJECT_NAME, entity=\"tronchonleo\")\n",
        "wandb.watch(ft_model, criterion, log=\"all\", log_freq=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9csy5ap5-GFp"
      },
      "source": [
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(config.FT_NUM_EPOCHS):\n",
        "\n",
        "  #Train model. Return losses and accuracy \n",
        "  loss, val_loss, val_accuracy = train_FT(train_loader, val_loader, ft_model, optimizer, criterion, config.DEVICE, scaler, config.LAMBDA )\n",
        "\n",
        "  #Display results\n",
        "  print(f\"Loss epoch {epoch}: \", loss.item())\n",
        "  print(f\"Validation Loss epoch {epoch}: \", val_loss.item())\n",
        "  print(f\"Validation Accuracy epoch {epoch}: \", val_accuracy)\n",
        "  wandb.log({\"loss\": loss.item(), \n",
        "             \"epoch\": epoch,\n",
        "             \"val_loss\": val_loss.item(),\n",
        "             \"val_accuracy\": val_accuracy}\n",
        "            )\n",
        "  \n",
        "  #Save best fine-tuned model\n",
        "  if (config.SAVE_CHECKPOINT and val_accuracy > best_accuracy):\n",
        "    best_accuracy = val_accuracy\n",
        "    save_checkpoint(ft_model, optimizer, filename= config.CHECKPOINT_FT_FILENAME)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}